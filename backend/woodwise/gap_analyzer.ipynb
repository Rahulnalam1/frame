{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gap_analyzer.py\n",
    "import os\n",
    "from supabase import create_client, Client\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "672935e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://shpsvnybkezljaoippva.supabase.co\n"
     ]
    }
   ],
   "source": [
    "# Initialize Supabase\n",
    "\n",
    "# SUPABASE_URL = os.getenv(\"SUPABASE_URL\", )\n",
    "# SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\", )\n",
    "print(SUPABASE_URL)\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77735081",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fetch_all_videos_with_summaries() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch all videos and their summaries from Supabase.\n",
    "    Returns a DataFrame with video metadata and aggregated summaries.\n",
    "    \"\"\"\n",
    "    print(\"Fetching videos from Supabase...\")\n",
    "\n",
    "    # Fetch videos\n",
    "    videos_response = supabase.table('videos').select('*').execute()\n",
    "    videos = videos_response.data\n",
    "\n",
    "    # Fetch all summaries\n",
    "    summaries_response = supabase.table(\n",
    "        'video_summaries').select('*').execute()\n",
    "    summaries = summaries_response.data\n",
    "\n",
    "    # Group summaries by video_id\n",
    "    summaries_by_video = {}\n",
    "    for summary in summaries:\n",
    "        video_id = summary['video_id']\n",
    "        if video_id not in summaries_by_video:\n",
    "            summaries_by_video[video_id] = []\n",
    "        summaries_by_video[video_id].append(summary)\n",
    "\n",
    "    # Combine data\n",
    "    video_data = []\n",
    "    for video in videos:\n",
    "        video_id = video['id']\n",
    "        video_summaries = summaries_by_video.get(video_id, [])\n",
    "\n",
    "        video_data.append({\n",
    "            'video_id': video_id,\n",
    "            'title': video.get('title', ''),\n",
    "            'duration': video['duration'],\n",
    "            'status': video['status'],\n",
    "            'key_topics': video.get('key_topics', ''),\n",
    "            'frame_interval': video['frame_interval'],\n",
    "            'total_frames': video['total_frames'],\n",
    "            'created_at': video['created_at'],\n",
    "            'summaries': video_summaries\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(video_data)\n",
    "    print(f\"Fetched {len(df)} videos with summaries\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_duration(duration_str: str) -> float:\n",
    "    \"\"\"\n",
    "    Parse duration string (e.g., '5:30', '1:15:45') to seconds.\n",
    "    \"\"\"\n",
    "    parts = duration_str.split(':')\n",
    "    if len(parts) == 2:  # MM:SS\n",
    "        return int(parts[0]) * 60 + int(parts[1])\n",
    "    elif len(parts) == 3:  # HH:MM:SS\n",
    "        return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def extract_all_descriptions(summaries: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate all frame descriptions into a single text.\n",
    "    \"\"\"\n",
    "    if not summaries:\n",
    "        return \"\"\n",
    "    return \" \".join([s['description'] for s in summaries])\n",
    "\n",
    "\n",
    "def calculate_scene_changes(summaries: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Estimate scene changes by comparing consecutive descriptions.\n",
    "    Simple heuristic: count when description similarity drops.\n",
    "    \"\"\"\n",
    "    if len(summaries) < 2:\n",
    "        return 0\n",
    "\n",
    "    changes = 0\n",
    "    for i in range(1, len(summaries)):\n",
    "        prev_desc = set(summaries[i-1]['description'].lower().split())\n",
    "        curr_desc = set(summaries[i]['description'].lower().split())\n",
    "\n",
    "        # If less than 30% overlap, consider it a scene change\n",
    "        if len(prev_desc) > 0:\n",
    "            overlap = len(prev_desc & curr_desc) / len(prev_desc)\n",
    "            if overlap < 0.3:\n",
    "                changes += 1\n",
    "\n",
    "    return changes\n",
    "\n",
    "\n",
    "def extract_topics_from_text(text: str, top_n: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract key topics/keywords from text using simple frequency.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Remove common stop words\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "                  'is', 'was', 'are', 'were', 'has', 'have', 'had', 'this', 'that',\n",
    "                  'with', 'from', 'by', 'of', 'as', 'it', 'be', 'can', 'will'}\n",
    "\n",
    "    words = text.lower().split()\n",
    "    filtered_words = [w for w in words if w not in stop_words and len(w) > 3]\n",
    "\n",
    "    word_counts = Counter(filtered_words)\n",
    "    return [word for word, count in word_counts.most_common(top_n)]\n",
    "\n",
    "\n",
    "# gap_analyzer.py (continued)\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer features from raw video data for WoodWide clustering.\n",
    "    \"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        summaries = row['summaries']\n",
    "        duration_seconds = parse_duration(row['duration'])\n",
    "\n",
    "        # Concatenate all descriptions\n",
    "        all_descriptions = extract_all_descriptions(summaries)\n",
    "\n",
    "        # Extract topics\n",
    "        topics_from_desc = extract_topics_from_text(all_descriptions, top_n=5)\n",
    "        topics_from_key = row['key_topics'].split(\n",
    "            ',') if row['key_topics'] else []\n",
    "        all_topics = list(set(topics_from_desc + topics_from_key))\n",
    "\n",
    "        # Feature calculations\n",
    "        feature_dict = {\n",
    "            'video_id': row['video_id'],\n",
    "            'title': row['title'],\n",
    "\n",
    "            # Duration features\n",
    "            'duration_seconds': duration_seconds,\n",
    "            'duration_minutes': duration_seconds / 60,\n",
    "\n",
    "            # Frame density\n",
    "            'total_frames': row['total_frames'],\n",
    "            'frames_per_minute': row['total_frames'] / (duration_seconds / 60) if duration_seconds > 0 else 0,\n",
    "\n",
    "            # Content complexity\n",
    "            'unique_topics_count': len(all_topics),\n",
    "            'topic_density': len(all_topics) / (duration_seconds / 60) if duration_seconds > 0 else 0,\n",
    "\n",
    "            # Description analysis\n",
    "            'avg_description_length': np.mean([len(s['description']) for s in summaries]) if summaries else 0,\n",
    "            'total_description_length': sum([len(s['description']) for s in summaries]),\n",
    "            'description_variance': np.var([len(s['description']) for s in summaries]) if len(summaries) > 1 else 0,\n",
    "\n",
    "            # Scene dynamics\n",
    "            'scene_changes': calculate_scene_changes(summaries),\n",
    "            'scene_change_rate': calculate_scene_changes(summaries) / (duration_seconds / 60) if duration_seconds > 0 else 0,\n",
    "\n",
    "            # Pacing\n",
    "            'frame_interval': row['frame_interval'],\n",
    "\n",
    "            # Store topics as JSON string for later analysis\n",
    "            'topics_json': json.dumps(all_topics),\n",
    "            'key_topics': row['key_topics'],\n",
    "        }\n",
    "\n",
    "        features.append(feature_dict)\n",
    "\n",
    "    features_df = pd.DataFrame(features)\n",
    "    print(\n",
    "        f\"Engineered {len(features_df.columns)} features for {len(features_df)} videos\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_topic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create additional features based on topic patterns.\n",
    "    This helps identify content gaps.\n",
    "    \"\"\"\n",
    "    print(\"Creating topic-based features...\")\n",
    "\n",
    "    # Extract all unique topics across all videos\n",
    "    all_topics = []\n",
    "    for topics_json in df['topics_json']:\n",
    "        all_topics.extend(json.loads(topics_json))\n",
    "\n",
    "    # Get top 20 most common topics\n",
    "    topic_counts = Counter(all_topics)\n",
    "    top_topics = [topic for topic, count in topic_counts.most_common(20)]\n",
    "\n",
    "    print(f\"Top topics in your content: {top_topics[:10]}\")\n",
    "\n",
    "    # Create binary features for each top topic\n",
    "    for topic in top_topics:\n",
    "        df[f'has_{topic}'] = df['topics_json'].apply(\n",
    "            lambda x: 1 if topic in json.loads(x) else 0\n",
    "        )\n",
    "\n",
    "    # Topic diversity score (entropy-like measure)\n",
    "    df['topic_diversity'] = df['topics_json'].apply(\n",
    "        lambda x: len(set(json.loads(x)))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_clustering_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare final dataset for WoodWide clustering.\n",
    "    Select only numeric features suitable for clustering.\n",
    "    \"\"\"\n",
    "    print(\"Preparing clustering dataset...\")\n",
    "\n",
    "    # Select numeric features for clustering\n",
    "    clustering_features = [\n",
    "        'duration_minutes',\n",
    "        'frames_per_minute',\n",
    "        'unique_topics_count',\n",
    "        'topic_density',\n",
    "        'avg_description_length',\n",
    "        'description_variance',\n",
    "        'scene_change_rate',\n",
    "        'topic_diversity',\n",
    "    ]\n",
    "\n",
    "    # Add topic binary features (has_*)\n",
    "    topic_cols = [col for col in df.columns if col.startswith('has_')]\n",
    "    clustering_features.extend(topic_cols)\n",
    "\n",
    "    # Create final dataset\n",
    "    cluster_df = df[['video_id', 'title'] + clustering_features].copy()\n",
    "\n",
    "    # Handle any NaN values\n",
    "    cluster_df = cluster_df.fillna(0)\n",
    "\n",
    "    print(f\"Clustering dataset shape: {cluster_df.shape}\")\n",
    "    print(f\"Features: {clustering_features[:5]}...\")\n",
    "\n",
    "    return cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0612a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from woodwide import WoodWide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d59efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gap_analyzer.py (continued)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, filename: str) -> str:\n",
    "    \"\"\"Save DataFrame to CSV file.\"\"\"\n",
    "    filepath = f\"/tmp/{filename}\"\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def train_content_gap_model(\n",
    "    features_df: pd.DataFrame,\n",
    "    api_key: str,\n",
    "    base_url: str = \"https://beta.woodwide.ai/\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train a WoodWide clustering model to identify content patterns.\n",
    "    \n",
    "    Returns:\n",
    "        model_id: The trained model ID\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING WOODWIDE CLUSTERING MODEL\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Initialize WoodWide client\n",
    "    client = WoodWide(api_key=api_key, base_url=base_url)\n",
    "    \n",
    "    # Prepare clustering dataset (only numeric features)\n",
    "    cluster_df = prepare_clustering_dataset(features_df)\n",
    "    \n",
    "    # Save to CSV (remove video_id and title for training)\n",
    "    training_df = cluster_df.drop(['video_id', 'title'], axis=1)\n",
    "    csv_path = save_to_csv(training_df, 'content_clustering.csv')\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Upload dataset\n",
    "        print(\"Step 1: Uploading dataset to WoodWide...\")\n",
    "        dataset_name = f\"content_patterns_{int(time.time())}\"\n",
    "        \n",
    "        with open(csv_path, 'rb') as f:\n",
    "            dataset = client.api.datasets.upload(\n",
    "                file=f,\n",
    "                name=dataset_name,\n",
    "                overwrite=True\n",
    "            )\n",
    "        \n",
    "        dataset_id = dataset.id\n",
    "        print(f\"âœ“ Dataset uploaded. ID: {dataset_id}\\n\")\n",
    "        \n",
    "        # Step 2: Train clustering model\n",
    "        print(\"Step 2: Training clustering model...\")\n",
    "        model_name = f\"gap_analyzer_{int(time.time())}\"\n",
    "        \n",
    "        # Use raw HTTP request for clustering endpoint\n",
    "        response = client._client.post(\n",
    "            \"/api/models/clustering/train\",\n",
    "            params={\"dataset_name\": dataset_name},\n",
    "            data={\n",
    "                \"model_name\": model_name,\n",
    "                \"overwrite\": \"true\",\n",
    "            },\n",
    "            headers=client.auth_headers,\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error starting training: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            raise Exception(\"Training failed to start\")\n",
    "        \n",
    "        response_json = response.json()\n",
    "        model_id = response_json.get(\"id\")\n",
    "        \n",
    "        if not model_id:\n",
    "            raise Exception(\"No model ID returned\")\n",
    "        \n",
    "        print(f\"âœ“ Training started. Model ID: {model_id}\\n\")\n",
    "        \n",
    "        # Step 3: Wait for training to complete\n",
    "        print(\"Step 3: Waiting for training to complete...\")\n",
    "        start_time = time.time()\n",
    "        timeout = 600  # 10 minutes\n",
    "        \n",
    "        while True:\n",
    "            model = client.api.models.retrieve(model_id)\n",
    "            training_status = model.training_status\n",
    "            \n",
    "            if training_status == \"COMPLETE\":\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"âœ“ Training complete! (took {elapsed:.2f}s)\\n\")\n",
    "                break\n",
    "            elif training_status == \"FAILED\":\n",
    "                print(\"âœ— Training failed!\")\n",
    "                print(model)\n",
    "                raise Exception(\"Training failed\")\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            if elapsed >= timeout:\n",
    "                raise Exception(f\"Training timeout after {timeout}s\")\n",
    "            \n",
    "            print(f\"  Status: {training_status}... ({elapsed:.0f}s elapsed)\")\n",
    "            time.sleep(5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL TRAINING SUCCESSFUL\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return model_id\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if os.path.exists(csv_path):\n",
    "            os.remove(csv_path)\n",
    "\n",
    "def run_clustering_inference(\n",
    "    model_id: str,\n",
    "    features_df: pd.DataFrame,\n",
    "    api_key: str,\n",
    "    base_url: str = \"https://beta.woodwide.ai/\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run inference on the clustering model to assign clusters.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with cluster assignments and cluster descriptions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RUNNING CLUSTERING INFERENCE\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    client = WoodWide(api_key=api_key, base_url=base_url)\n",
    "    \n",
    "    # Prepare same features as training\n",
    "    cluster_df = prepare_clustering_dataset(features_df)\n",
    "    \n",
    "    # Save inference dataset\n",
    "    inference_df = cluster_df.drop(['video_id', 'title'], axis=1)\n",
    "    csv_path = save_to_csv(inference_df, 'inference_data.csv')\n",
    "    \n",
    "    try:\n",
    "        # Upload inference dataset\n",
    "        print(\"Uploading inference dataset...\")\n",
    "        inference_dataset_name = f\"inference_{int(time.time())}\"\n",
    "        \n",
    "        with open(csv_path, 'rb') as f:\n",
    "            inference_dataset = client.api.datasets.upload(\n",
    "                file=f,\n",
    "                name=inference_dataset_name,\n",
    "                overwrite=True\n",
    "            )\n",
    "        \n",
    "        inference_dataset_id = inference_dataset.id\n",
    "        print(f\"âœ“ Dataset uploaded. ID: {inference_dataset_id}\\n\")\n",
    "        \n",
    "        # Run inference\n",
    "        print(\"Running clustering inference...\")\n",
    "        result = client.api.models.clustering.infer(\n",
    "            model_id=model_id,\n",
    "            dataset_id=inference_dataset_id\n",
    "        )\n",
    "        \n",
    "        print(\"âœ“ Inference complete!\\n\")\n",
    "        \n",
    "        # Parse WoodWide clustering response\n",
    "        # Format: {\"cluster_label\": {\"0\": 2, \"1\": 3, ...}, \"cluster_descriptions\": {...}}\n",
    "        if hasattr(result, 'model_dump'):\n",
    "            result_dict = result.model_dump()\n",
    "        elif isinstance(result, dict):\n",
    "            result_dict = result\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected result type: {type(result)}\")\n",
    "        \n",
    "        # Extract cluster labels (mapping: row_index -> cluster_id)\n",
    "        cluster_label_dict = result_dict.get('cluster_label', {})\n",
    "        cluster_descriptions = result_dict.get('cluster_descriptions', {})\n",
    "        \n",
    "        if not cluster_label_dict:\n",
    "            raise ValueError(\"No cluster_label found in response\")\n",
    "        \n",
    "        # Convert cluster labels to list (sorted by index)\n",
    "        clusters = []\n",
    "        for i in range(len(cluster_df)):\n",
    "            cluster_id = cluster_label_dict.get(str(i))\n",
    "            if cluster_id is None:\n",
    "                raise ValueError(f\"Missing cluster assignment for row {i}\")\n",
    "            clusters.append(cluster_id)\n",
    "        \n",
    "        print(f\"Successfully extracted {len(clusters)} cluster assignments\")\n",
    "        print(f\"Found {len(cluster_descriptions)} unique clusters\\n\")\n",
    "        \n",
    "        # Create final dataframe with cluster assignments\n",
    "        final_df = features_df[['video_id', 'title']].copy()\n",
    "        final_df['cluster'] = clusters\n",
    "        \n",
    "        # Add cluster descriptions\n",
    "        final_df['cluster_description'] = final_df['cluster'].apply(\n",
    "            lambda x: cluster_descriptions.get(str(x), 'No description')\n",
    "        )\n",
    "        \n",
    "        # Add key metrics for analysis\n",
    "        final_df['duration_minutes'] = features_df['duration_minutes']\n",
    "        final_df['unique_topics'] = features_df['unique_topics_count']\n",
    "        final_df['scene_change_rate'] = features_df['scene_change_rate']\n",
    "        final_df['topics'] = features_df['topics_json']\n",
    "        \n",
    "        print(f\"Cluster distribution:\")\n",
    "        print(final_df['cluster'].value_counts().sort_index())\n",
    "        print()\n",
    "        \n",
    "        # Save cluster descriptions separately\n",
    "        cluster_info = {\n",
    "            'cluster_descriptions': cluster_descriptions,\n",
    "            'cluster_counts': final_df['cluster'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        with open('cluster_info.json', 'w') as f:\n",
    "            json.dump(cluster_info, f, indent=2)\n",
    "        print(\"Cluster descriptions saved to cluster_info.json\\n\")\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    finally:\n",
    "        if os.path.exists(csv_path):\n",
    "            os.remove(csv_path)\n",
    "def run_clustering_inference_debug(\n",
    "    model_id: str,\n",
    "    features_df: pd.DataFrame,\n",
    "    api_key: str,\n",
    "    base_url: str = \"https://beta.woodwide.ai/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Debug version to see what WoodWide returns.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    \n",
    "    client = WoodWide(api_key=api_key, base_url=base_url)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    cluster_df = prepare_clustering_dataset(features_df)\n",
    "    inference_df = cluster_df.drop(['video_id', 'title'], axis=1)\n",
    "    csv_path = save_to_csv(inference_df, 'inference_data.csv')\n",
    "    \n",
    "    try:\n",
    "        # Upload dataset\n",
    "        print(\"Uploading dataset...\")\n",
    "        with open(csv_path, 'rb') as f:\n",
    "            inference_dataset = client.api.datasets.upload(\n",
    "                file=f,\n",
    "                name=f\"debug_inference_{int(time.time())}\",\n",
    "                overwrite=True\n",
    "            )\n",
    "        \n",
    "        inference_dataset_id = inference_dataset.id\n",
    "        print(f\"Dataset ID: {inference_dataset_id}\")\n",
    "        \n",
    "        # Make raw API call to see response\n",
    "        print(\"\\nMaking raw API call...\")\n",
    "        url = f\"{base_url}/api/models/clustering/{model_id}/infer?dataset_id={inference_dataset_id}\"\n",
    "        \n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers)\n",
    "        \n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        print(f\"Response Headers: {dict(response.headers)}\")\n",
    "        print(f\"\\nRaw Response Text:\")\n",
    "        print(response.text)\n",
    "        print()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                json_response = response.json()\n",
    "                print(f\"Parsed JSON:\")\n",
    "                print(json.dumps(json_response, indent=2))\n",
    "            except:\n",
    "                print(\"Could not parse as JSON\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    finally:\n",
    "        if os.path.exists(csv_path):\n",
    "            os.remove(csv_path)\n",
    "\n",
    "# Run this instead\n",
    "# debug_response = run_clustering_inference_debug(\n",
    "#     model_id,\n",
    "#     features_df,\n",
    "#     api_key=args[\"api_key\"]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc6138e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gap_analyzer.py (continued)\n",
    "\n",
    "def analyze_clusters(clustered_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze clusters to identify content patterns and gaps.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLUSTER ANALYSIS\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    analysis = {\n",
    "        'total_videos': len(clustered_df),\n",
    "        'num_clusters': clustered_df['cluster'].nunique(),\n",
    "        'clusters': {}\n",
    "    }\n",
    "    \n",
    "    for cluster_id in sorted(clustered_df['cluster'].unique()):\n",
    "        cluster_videos = clustered_df[clustered_df['cluster'] == cluster_id]\n",
    "        \n",
    "        # Aggregate topics in this cluster\n",
    "        all_topics = []\n",
    "        for topics_json in cluster_videos['topics']:\n",
    "            all_topics.extend(json.loads(topics_json))\n",
    "        topic_counts = Counter(all_topics)\n",
    "        top_topics = [t for t, c in topic_counts.most_common(5)]\n",
    "        \n",
    "        cluster_info = {\n",
    "            'video_count': len(cluster_videos),\n",
    "            'percentage': len(cluster_videos) / len(clustered_df) * 100,\n",
    "            'avg_duration': cluster_videos['duration_minutes'].mean(),\n",
    "            'avg_topics': cluster_videos['unique_topics'].mean(),\n",
    "            'avg_scene_change_rate': cluster_videos['scene_change_rate'].mean(),\n",
    "            'top_topics': top_topics,\n",
    "            'video_titles': cluster_videos['title'].tolist()\n",
    "        }\n",
    "        \n",
    "        analysis['clusters'][f'cluster_{cluster_id}'] = cluster_info\n",
    "        \n",
    "        # Print cluster summary\n",
    "        print(f\"Cluster {cluster_id}: ({cluster_info['video_count']} videos, {cluster_info['percentage']:.1f}%)\")\n",
    "        print(f\"  Avg Duration: {cluster_info['avg_duration']:.1f} min\")\n",
    "        print(f\"  Top Topics: {', '.join(top_topics[:3])}\")\n",
    "        print(f\"  Sample Videos:\")\n",
    "        for title in cluster_info['video_titles'][:3]:\n",
    "            print(f\"    - {title}\")\n",
    "        print()\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def identify_content_gaps(analysis: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Identify content gaps based on cluster distribution.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CONTENT GAP IDENTIFICATION\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    clusters = analysis['clusters']\n",
    "    \n",
    "    # Find underrepresented clusters\n",
    "    avg_percentage = 100 / len(clusters)\n",
    "    \n",
    "    gaps = {\n",
    "        'underrepresented_patterns': [],\n",
    "        'overrepresented_patterns': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    for cluster_name, cluster_data in clusters.items():\n",
    "        percentage = cluster_data['percentage']\n",
    "        \n",
    "        if percentage < avg_percentage * 0.5:  # Less than 50% of average\n",
    "            gaps['underrepresented_patterns'].append({\n",
    "                'cluster': cluster_name,\n",
    "                'current_count': cluster_data['video_count'],\n",
    "                'percentage': percentage,\n",
    "                'pattern': {\n",
    "                    'duration': cluster_data['avg_duration'],\n",
    "                    'topics': cluster_data['top_topics']\n",
    "                }\n",
    "            })\n",
    "        elif percentage > avg_percentage * 1.5:  # More than 150% of average\n",
    "            gaps['overrepresented_patterns'].append({\n",
    "                'cluster': cluster_name,\n",
    "                'current_count': cluster_data['video_count'],\n",
    "                'percentage': percentage,\n",
    "                'pattern': {\n",
    "                    'duration': cluster_data['avg_duration'],\n",
    "                    'topics': cluster_data['top_topics']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Generate recommendations\n",
    "    for gap in gaps['underrepresented_patterns']:\n",
    "        recommendation = f\"Create more {gap['pattern']['duration']:.0f}-minute videos about: {', '.join(gap['pattern']['topics'][:3])}\"\n",
    "        gaps['recommendations'].append(recommendation)\n",
    "    \n",
    "    # Print gaps\n",
    "    print(\"ðŸ”´ UNDERREPRESENTED CONTENT PATTERNS:\")\n",
    "    for gap in gaps['underrepresented_patterns']:\n",
    "        print(f\"\\n  {gap['cluster']}: Only {gap['current_count']} videos ({gap['percentage']:.1f}%)\")\n",
    "        print(f\"    Typical Duration: {gap['pattern']['duration']:.1f} minutes\")\n",
    "        print(f\"    Topics: {', '.join(gap['pattern']['topics'])}\")\n",
    "    \n",
    "    print(\"\\n\\nðŸŸ¢ OVERREPRESENTED CONTENT PATTERNS:\")\n",
    "    for gap in gaps['overrepresented_patterns']:\n",
    "        print(f\"\\n  {gap['cluster']}: {gap['current_count']} videos ({gap['percentage']:.1f}%)\")\n",
    "        print(f\"    Typical Duration: {gap['pattern']['duration']:.1f} minutes\")\n",
    "        print(f\"    Topics: {', '.join(gap['pattern']['topics'])}\")\n",
    "    \n",
    "    print(\"\\n\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(gaps['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73961663",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"api_key\": \"sk_QXg4S5ZpXNkonM9i-QB2Td5pEWCcmnZuLGYZT5YYtEE\",\n",
        "    \"train\": True,\n",
    "    \"model_id\": \"Run Test 1\",\n",
    "    \"output\": \"gap_analysis.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de59b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching videos from Supabase...\n",
      "Fetched 14 videos with summaries\n"
     ]
    }
   ],
   "source": [
    "df = fetch_all_videos_with_summaries()\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"No videos found in database!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f9ab8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features...\n",
      "Engineered 16 features for 14 videos\n",
      "Creating topic-based features...\n",
      "Top topics in your content: ['image', 'white', 'scene', 'black', 'boat', 'The image depicts a scene from a commercial for Pringles potato chips. The setting is a kitchen', ' The image shows...', ' including a stove', ' a refrigerator', 'potato']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Engineer features\n",
    "features_df = engineer_features(df)\n",
    "features_df = create_topic_features(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c175ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to video_features.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.to_csv('video_features.csv', index=False)\n",
    "print(f\"Features saved to video_features.csv\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0ac1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "\n",
      "==================================================\n",
      "TRAINING WOODWIDE CLUSTERING MODEL\n",
      "==================================================\n",
      "\n",
      "Preparing clustering dataset...\n",
      "Clustering dataset shape: (14, 30)\n",
      "Features: ['duration_minutes', 'frames_per_minute', 'unique_topics_count', 'topic_density', 'avg_description_length']...\n",
      "Saved to /tmp/content_clustering.csv\n",
      "Step 1: Uploading dataset to WoodWide...\n",
      "âœ“ Dataset uploaded. ID: M7HBmqtvXfI5G0ANnHNG\n",
      "\n",
      "Step 2: Training clustering model...\n",
      "âœ“ Training started. Model ID: 99qKv5bY7hcobPraftoj\n",
      "\n",
      "Step 3: Waiting for training to complete...\n",
      "  Status: PENDING... (0s elapsed)\n",
      "  Status: PENDING... (5s elapsed)\n",
      "  Status: PENDING... (11s elapsed)\n",
      "  Status: PENDING... (16s elapsed)\n",
      "  Status: PENDING... (21s elapsed)\n",
      "  Status: PENDING... (26s elapsed)\n",
      "  Status: PENDING... (32s elapsed)\n",
      "  Status: RUNNING... (37s elapsed)\n",
      "  Status: RUNNING... (42s elapsed)\n",
      "  Status: RUNNING... (47s elapsed)\n",
      "  Status: RUNNING... (52s elapsed)\n",
      "âœ“ Training complete! (took 57.68s)\n",
      "\n",
      "\n",
      "==================================================\n",
      "MODEL TRAINING SUCCESSFUL\n",
      "==================================================\n",
      "\n",
      "\n",
      "âœ“ Model trained successfully!\n",
      "Model ID: 99qKv5bY7hcobPraftoj\n",
      "Save this ID for future inference!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Train or use existing model\n",
    "if True:\n",
    "    print(\"Training model...\")\n",
    "    model_id = train_content_gap_model(\n",
    "        features_df,\n",
    "        api_key=args[\"api_key\"]\n",
    "    )\n",
    "    print(f\"\\nâœ“ Model trained successfully!\")\n",
    "    print(f\"Model ID: {model_id}\")\n",
    "    print(f\"Save this ID for future inference!\\n\")\n",
    "else:\n",
    "    if not args.model_id:\n",
    "        print(\"Error: Either --train or --model-id must be provided\")\n",
    "    model_id = args.model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcb23047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RUNNING CLUSTERING INFERENCE\n",
      "==================================================\n",
      "\n",
      "Preparing clustering dataset...\n",
      "Clustering dataset shape: (14, 30)\n",
      "Features: ['duration_minutes', 'frames_per_minute', 'unique_topics_count', 'topic_density', 'avg_description_length']...\n",
      "Saved to /tmp/inference_data.csv\n",
      "Uploading inference dataset...\n",
      "âœ“ Dataset uploaded. ID: GsUOh1dgGKfogAi15bde\n",
      "\n",
      "Running clustering inference...\n",
      "âœ“ Inference complete!\n",
      "\n",
      "Successfully extracted 14 cluster assignments\n",
      "Found 8 unique clusters\n",
      "\n",
      "Cluster distribution:\n",
      "cluster\n",
      "0    4\n",
      "1    2\n",
      "2    3\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cluster descriptions saved to cluster_info.json\n",
      "\n",
      "Clustered videos saved to clustered_videos.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustered_df = run_clustering_inference(\n",
    "    model_id,\n",
    "    features_df,\n",
    "    api_key=args[\"api_key\"]\n",
    ")\n",
    "# debug_response = run_clustering_inference_debug(\n",
    "#     model_id,\n",
    "#     features_df,\n",
    "#     api_key=args[\"api_key\"]\n",
    "# )\n",
    "\n",
    "# Save clustered results\n",
    "clustered_df.to_csv('clustered_videos.csv', index=False)\n",
    "print(f\"Clustered videos saved to clustered_videos.csv\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3181cf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CLUSTER ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Cluster 0: (4 videos, 28.6%)\n",
      "  Avg Duration: 0.0 min\n",
      "  Top Topics: \n",
      "  Sample Videos:\n",
      "    - TEST MODEL??\n",
      "    - TEST MODEL??\n",
      "    - TEST MODEL??\n",
      "\n",
      "Cluster 1: (2 videos, 14.3%)\n",
      "  Avg Duration: 0.5 min\n",
      "  Top Topics: The image depicts a scene from a commercial for Pringles potato chips. The setting is a kitchen,  The image shows..., image\n",
      "  Sample Videos:\n",
      "    - Pringles Ad\n",
      "    - Pringles Ad 2\n",
      "\n",
      "Cluster 2: (3 videos, 21.4%)\n",
      "  Avg Duration: 0.2 min\n",
      "  Top Topics: image, white, boat\n",
      "  Sample Videos:\n",
      "    - Rahul Motorcycle\n",
      "    - Animation Video\n",
      "    - Model 123\n",
      "\n",
      "Cluster 3: (1 videos, 7.1%)\n",
      "  Avg Duration: 0.1 min\n",
      "  Top Topics:  wearing a helmet and a dark shirt. The motorcycle is parked on the street, motorcycle,  also wearing a helmet and sunglasses. In the background\n",
      "  Sample Videos:\n",
      "    - Rahul Motorcycle\n",
      "\n",
      "Cluster 4: (1 videos, 7.1%)\n",
      "  Avg Duration: 0.0 min\n",
      "  Top Topics: image,  which is part of the house where the movie is set. The hallway is well-lit,  portrayed by the actor Edgardo Saverin. The scene takes place in a hallway\n",
      "  Sample Videos:\n",
      "    - None\n",
      "\n",
      "Cluster 5: (1 videos, 7.1%)\n",
      "  Avg Duration: 0.0 min\n",
      "  Top Topics:  while the tilted can has a matte finish. The upright..., image,  while the tilted can is positioned to the left of the upright can. Both cans are red and white in color\n",
      "  Sample Videos:\n",
      "    - Coca-Cola | For Everyone :30\n",
      "\n",
      "Cluster 6: (1 videos, 7.1%)\n",
      "  Avg Duration: 16.3 min\n",
      "  Top Topics: \n",
      "  Sample Videos:\n",
      "    - iPhone Video\n",
      "\n",
      "Cluster 7: (1 videos, 7.1%)\n",
      "  Avg Duration: 0.5 min\n",
      "  Top Topics: \n",
      "  Sample Videos:\n",
      "    - Animation Video\n",
      "\n",
      "\n",
      "==================================================\n",
      "CONTENT GAP IDENTIFICATION\n",
      "==================================================\n",
      "\n",
      "ðŸ”´ UNDERREPRESENTED CONTENT PATTERNS:\n",
      "\n",
      "\n",
      "ðŸŸ¢ OVERREPRESENTED CONTENT PATTERNS:\n",
      "\n",
      "  cluster_0: 4 videos (28.6%)\n",
      "    Typical Duration: 0.0 minutes\n",
      "    Topics: \n",
      "\n",
      "  cluster_2: 3 videos (21.4%)\n",
      "    Typical Duration: 0.2 minutes\n",
      "    Topics: image, white, boat, idefics3forconditionalgeneration.__init__(), error\n",
      "\n",
      "\n",
      "ðŸ’¡ RECOMMENDATIONS:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis = analyze_clusters(clustered_df)\n",
    "gaps = identify_content_gaps(analysis)\n",
    "\n",
    "# Step 6: Save final results\n",
    "results = {\n",
    "    'model_id': model_id,\n",
    "    'analysis': analysis,\n",
    "    'gaps': gaps,\n",
    "    'clustered_videos': clustered_df.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "with open(\"gap_analysis.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "#print(f\"\\nâœ“ Analysis complete! Results saved to {args.output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a81de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main execution function.\n",
    "\"\"\"\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Content Gap Analyzer using WoodWide AI')\n",
    "# parser.add_argument('--api-key', required=True, help='WoodWide API key')\n",
    "# parser.add_argument('--supabase-url', required=True, help='Supabase URL')\n",
    "# parser.add_argument('--supabase-key', required=True, help='Supabase API key')\n",
    "# parser.add_argument('--train', action='store_true', help='Train a new model')\n",
    "# parser.add_argument('--model-id', help='Existing model ID for inference')\n",
    "# parser.add_argument('--output', default='gap_analysis.json', help='Output file for results')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "# os.environ['SUPABASE_URL'] = args.supabase_url\n",
    "# os.environ['SUPABASE_KEY'] = args.supabase_key\n",
    "\n",
    "# Step 1: Fetch data from Supabase\n",
    "\n",
    "\n",
    "\n",
    "# Save features for inspection\n",
    "\n",
    "\n",
    "# Step 4: Run clustering inference\n",
    "\n",
    "# Step 5: Analyze clusters and identify gaps\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
